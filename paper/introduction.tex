\section{Introduction}

The Grand Challenge 2015 problem focuses on processing New York city taxi trip data for the year 2013. The area considered is a square of 150km x  150km. The  square is divided into smaller units called cells and a taxi trip is considered as moving from one cell (pickup cell) to another cell (dropoff cell). A route is uniquely identified by the pickup cell and the dropoff cell. The dataset consists of 173 million events and solutions are expected to replay these data and generate events that meet the criteria of two defined queries. 

The first query is to find the events that change the top ten routes within last 30 minutes and emit the new route set details along the original event. The given dataset contains 899010 routes and the 30 minute time window for this query accounts around 10000 events assuming an equal event distribution. This creates the possibility of processing several thousand route counts especially during peak hours. In other words we need to evaluate several thousand route counts with each event and find 10 top routes. A similar analysis can be made for the profitable cells query as well. In addition to that it requires to calculate the median fare for each cell. 

\subsection{Research Challenges}

If we consider an application that must process the 173 million trips dataset within an year, it would be a trivial task and several simple implementations would satisfy that requirement. However if we consider an application that must process these data within minutes with the added constraint of minimal latency to process each event, we observe following research challenges.
\begin{enumerate}
	\item \textit{Handling thousands of values in real time :}  An algorithm with O(n) time complexity will not scale for thousands. How can we design an algorithm with O(log n) time complexity?
	\item \textit{Parallel evaluation :}  This application does not inherently support parallel evaluation. Partitioning route sets and cell sets triggers events for subsets, but application requires the events with whole sets. Therefore is it possible to partition data and still generate required results to support parallelism?  
\end{enumerate}

\subsection{Methodology}

We started our work seeking answers for the above challenges. First we evaluated doubly linked list and heap based approaches to build a data structure with O(log n) time complexity. After several evaluations, we developed a new data structure called \textit{NodeList} that utilizes both these concepts. In order to find the running median with O(log n) time complexity, we experimented with an algorithm which uses priority queue and the heap data structure we developed. We decided to use former since it gave better performance with smaller window sizes. We then focused on parallelizing the application and designed a strategy to aggregate the subset results to find the final results. We also conducted several experiments to evaluate the scalability of our solution. 


\subsection{Contributions}
We observe following key contributions, which can be used in solving future DEBS challenges as well.
\begin{enumerate}
	\item We have designed a set of  generic data structures capable of inserting, updating, removing and retrieving top values (order can be defined using a comparator function) with O(log n) time complexity. 
	\item Using our data structures we have developed an algorithm to find the running median with O(log n) time complexity. One of the key challenges of DEBS 2014 \cite{jerzak2014debs}  was to find the median of a large time window. We believe our solution is useful in such situations.
	\item We have developed a scalable parallel evaluation technique to process queries efficiently. 
\end{enumerate}

\subsection{Paper Organization}
The remainder of this paper is organized as follows. Section 2 provides an in-depth description about the internal data structures and the algorithms used to process the events. Section 3 describes our scalable design to evaluate queries and associated problems. Section 4 proves the scalability of our solution with experimental results. Section 5 discusses the related work including some last year solutions. Section 6 summarizes key concepts and discusses their further applications. 
